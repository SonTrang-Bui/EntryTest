{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f1c61d6",
   "metadata": {},
   "source": [
    "# Recommendation System from Transaction History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf49106",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to create a recommendation system from transaction history. The completed model will allow users to obtain suggested items for each user presented in the database. The data is obtained from Kaggle on the following URL: https://www.kaggle.com/datasets/vipin20/transaction-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e9d97b",
   "metadata": {},
   "source": [
    "The program used Spark under Python API as the engine to compute a relatively big volume of data (over 1 million records). However, Spark is only experienced on local mode in this notebook, so it cannot utilize the power of distributed system fully and, therefore, is limited in computationally intensive processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194a97d1",
   "metadata": {},
   "source": [
    "SparkSession is imported to start programming with Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb387148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69698e18",
   "metadata": {},
   "source": [
    "## 1. CSV File Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f720330",
   "metadata": {},
   "source": [
    "pyspark.sql.types is imported to contruct dataframe schema manually instead of setting using inferSchema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68052b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6db42af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_schema = StructType([StructField('UserID', StringType(), True),\n",
    "                                StructField('TransactionID', StringType(), True),\n",
    "                                StructField('TransactionTime', StringType(), True),\n",
    "                                StructField('ItemCode', StringType(), True),\n",
    "                                StructField('ItemDescription', StringType(), True),\n",
    "                                StructField('NumberOfItemPurchased', IntegerType(), True),\n",
    "                                StructField('CostPerItem', DoubleType(), True),\n",
    "                                StructField('Country', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bf61f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    " .builder\n",
    " .appName(\"Transaction\")\n",
    " .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "524f939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('transaction_data.csv', header = True, schema = transaction_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29052bcf",
   "metadata": {},
   "source": [
    "After inspection, it is reasonable to conclude that \"-1\", \"?\", and \"??\" are used to represent null values in selected columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad8219cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(\"-1\",None,['UserID','TransactionID', 'TransactionTime','ItemCode'])\n",
    "df = df.replace([\"?\", \"??\"],None,['ItemDescription','Country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "991f0fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, col, to_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56121900",
   "metadata": {},
   "source": [
    "In the dataset, ItemDescription is mostly recorded in uppercase. However, to maintain uniformity, the column is transformed into lowercase. TransactionTime is also brought to the correct data type, timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6283d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "df = df.withColumn(\"ItemDescription\", lower(col('ItemDescription')))\n",
    "df = df.withColumn(\"TransactionTime\", to_timestamp(col('TransactionTime'), \"EEE MMM dd HH:mm:ss zzz yyyy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d51ed6",
   "metadata": {},
   "source": [
    "## 2. Null Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0355420f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+---------------+-------------------+--------------------+---------------------+-----------+-------+\n",
      "|           UserID|TransactionID|TransactionTime|           ItemCode|     ItemDescription|NumberOfItemPurchased|CostPerItem|Country|\n",
      "+-----------------+-------------+---------------+-------------------+--------------------+---------------------+-----------+-------+\n",
      "|0.249266943342886|          0.0|            0.0|0.00515953785598689|0.002782755038207522|                  0.0|        0.0|    0.0|\n",
      "+-----------------+-------------+---------------+-------------------+--------------------+---------------------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, count\n",
    "\n",
    "df.select([(count(when(col(c).isNull(), c))/df.count()).alias(c) for c in df.columns]).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1556ee0c",
   "metadata": {},
   "source": [
    "When the null property of the dataset is studied, it is easy to see that there is a high proportion of UserID left unrecorded. As the aim of the notebook is to study the behavior of buyers, unknown users will disturb the process greatly. Imputation of this feature may introduce unwanted biases; therefore, these rows will be isolated from the dataset to study further. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4212c9",
   "metadata": {},
   "source": [
    "As ItemCode and ItemDescription can be related, their relationship is considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bdd36ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------------------+\n",
      "|ItemCode|count(DISTINCT ItemDescription)|\n",
      "+--------+-------------------------------+\n",
      "|  443604|                              1|\n",
      "|  493542|                              2|\n",
      "|  456057|                              1|\n",
      "|  492618|                              1|\n",
      "| 1529052|                              1|\n",
      "|  455070|                              1|\n",
      "|  449841|                              2|\n",
      "|  467271|                              1|\n",
      "|  489468|                              1|\n",
      "| 1893108|                              1|\n",
      "|  485457|                              2|\n",
      "|  493122|                              1|\n",
      "|  463554|                              1|\n",
      "|  434007|                              1|\n",
      "|  458388|                              1|\n",
      "| 1660407|                              1|\n",
      "|  494277|                              2|\n",
      "|    1512|                              0|\n",
      "|  489993|                              1|\n",
      "|  472143|                              1|\n",
      "+--------+-------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "df.groupBy('ItemCode').agg(countDistinct('ItemDescription')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b49e6d",
   "metadata": {},
   "source": [
    "From display, we can see that there are several ItemCode having more than 1 definitions or not at all. Therefore, it is not possible to map the ItemCode to ItemDesctiption and vice versa. As the proportion of blank records are low, they can be ignored in the following process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5edcc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2610496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_rows = df.filter(col('UserID').isNull() | col('ItemCode').isNull() | col('ItemDescription').isNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd282e3",
   "metadata": {},
   "source": [
    "## 3. Data Accuracy Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f87601f",
   "metadata": {},
   "source": [
    "The summary of two numerical columns are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9537948c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------+------------------+\n",
      "|summary|NumberOfItemPurchased|       CostPerItem|\n",
      "+-------+---------------------+------------------+\n",
      "|  count|               810086|            810086|\n",
      "|   mean|    36.30636747209555| 8.218449646086775|\n",
      "| stddev|    747.5887214570251|2665.3132368669712|\n",
      "|    min|              -242985|               0.0|\n",
      "|    25%|                    6|              1.73|\n",
      "|    50%|                   15|               2.7|\n",
      "|    75%|                   36|              5.18|\n",
      "|    max|               242985|        1696285.44|\n",
      "+-------+---------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.select('NumberOfItemPurchased', 'CostPerItem').summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "044d9bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.filter(col('CostPerItem') == 0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0189ba4",
   "metadata": {},
   "source": [
    "As we can see, the minimum value of CostPerItem is 0. The total records of this value is incosiderable compared to the dataset. Although, the CostPerItem being 0 may be caused by several reasons such as gift, vouchers, etc., their exact drives are not clear, and these numbers may introduce unexpected results later in the transformation. Therefore, these recorded are excluded from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcb62275",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.filter(col('CostPerItem') > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc893df",
   "metadata": {},
   "source": [
    "Another noticable trait is the nagative value of NumberOfItemPurchased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1e62dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-------------------+--------+--------------------+---------------------+-----------+--------------+\n",
      "|UserID|TransactionID|    TransactionTime|ItemCode|     ItemDescription|NumberOfItemPurchased|CostPerItem|       Country|\n",
      "+------+-------------+-------------------+--------+--------------------+---------------------+-----------+--------------+\n",
      "|300909|      6015757|2018-05-29 22:14:00|  466452|four hook  white ...|                   -3|        2.9|United Kingdom|\n",
      "|319683|      6036228|2018-06-16 16:28:00|  470883|regency cakestand...|                  -24|       17.6|United Kingdom|\n",
      "|321531|      5925150|2018-02-24 20:05:00|  446418|victorian sewing ...|                   -3|      15.12|United Kingdom|\n",
      "|260715|      6165940|2018-09-30 16:26:00|  488061|treasure tin gymk...|                   -3|       2.88|     Australia|\n",
      "|274869|      6004240|2018-05-19 17:35:00|  470883|regency cakestand...|                  -15|      15.12|United Kingdom|\n",
      "|331989|      6319203|2019-01-16 17:05:00|  493437|classic chrome bi...|                  -36|       2.01|United Kingdom|\n",
      "|331905|      6062958|2018-07-08 21:26:00|  475986|recipe box pantry...|                  -12|       4.08|United Kingdom|\n",
      "|293811|      6303946|2019-01-08 19:27:00|  463365|christmas pudding...|                   -3|       0.54|United Kingdom|\n",
      "|275373|      5991205|2018-05-06 16:57:00|  786387|ceramic cake desi...|                  -15|       2.06|United Kingdom|\n",
      "|337008|      6234602|2018-11-27 15:12:00| 1788423|black baroque wal...|                   -3|       17.6|United Kingdom|\n",
      "|263760|      6214043|2018-11-12 17:39:00| 1764630|english rose hot ...|                   -6|       5.87|       Germany|\n",
      "|316281|      5950318|2018-03-26 17:10:00| 1788738|victorian sewing kit|                 -144|       1.18|United Kingdom|\n",
      "|268359|      6107684|2018-08-13 18:11:00|  460509|jumbo bag pink vi...|                   -6|       2.88|        Poland|\n",
      "|287280|      5944587|2018-03-21 15:56:00|  435225|lunch bag red ret...|                 -300|       2.01|United Kingdom|\n",
      "|328818|      6021488|2018-06-04 12:55:00|  475167| mint kitchen scales|                   -3|      11.73|United Kingdom|\n",
      "|306726|      6189128|2018-10-20 17:56:00|  452319|red retrospot but...|                   -3|       6.84|United Kingdom|\n",
      "|265146|      6151079|2018-09-18 14:25:00|  462588|penny farthing bi...|                  -36|       0.58|       Germany|\n",
      "|285579|      6396148|2019-02-20 00:26:00|  447594|small glass heart...|                  -30|        2.9|United Kingdom|\n",
      "|322665|      6269835|2018-12-19 00:51:00|  460215|red  harmonica in...|                  -36|       1.73|United Kingdom|\n",
      "|321531|      6272233|2018-12-19 21:49:00|  460509|jumbo bag pink vi...|                   -3|       2.48|United Kingdom|\n",
      "+------+-------------+-------------------+--------+--------------------+---------------------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.where((col('NumberOfItemPurchased') < 0)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781768e3",
   "metadata": {},
   "source": [
    "These numbers can be interpreted as returned orders. To present the status of the transaction, TransactionCancel is added to detail this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e794167",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.withColumn('TransactionCancel', (col('NumberOfItemPurchased') < 0).cast('integer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42774597",
   "metadata": {},
   "source": [
    "## 4. Duplication Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4944e1",
   "metadata": {},
   "source": [
    "Duplicated recorded are removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ea8d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e126d3",
   "metadata": {},
   "source": [
    "## 5. Outlier Handiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35b7277",
   "metadata": {},
   "source": [
    "In this dataset, the outliers are detected mostly in numerical columns using Interquartile Range. Even though this is a straightforward method, a more complex procedure (Clustering, Isolation Forest, etc.) may assure a better detection of these points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6518363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr_outlier_treatment(dataframe, column, factor=1.5):\n",
    "    quantiles = dataframe.approxQuantile(column, [0.25, 0.75], 0.01)\n",
    "    q1, q3 = quantiles[0], quantiles[1]\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # Define the upper and lower bounds for outliers\n",
    "    lower_bound = q1 - factor * iqr\n",
    "    upper_bound = q3 + factor * iqr\n",
    "\n",
    "    # Filter outliers and update the DataFrame\n",
    "    dataframe = dataframe.filter((col(column) >= lower_bound) & (col(column) <= upper_bound))\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba003f8",
   "metadata": {},
   "source": [
    "There are several different ways to treat outliers, including transformation and imputation. In this notebook, these points are removed as they are inconsiderable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "817a9ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = iqr_outlier_treatment(new_df, 'NumberOfItemPurchased')\n",
    "new_df = iqr_outlier_treatment(new_df, 'CostPerItem')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d64761c",
   "metadata": {},
   "source": [
    "## 6. Exploration of ItemCode in each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c42d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.withColumn('Inventory Value', col('NumberOfItemPurchased') * col('CostPerItem'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4152022b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----+-------------------+------------------+\n",
      "|ItemCode|month|year|TotalPurchasedItems|      Total Prices|\n",
      "+--------+-----+----+-------------------+------------------+\n",
      "|  482349|   10|2018|               1044|           2098.44|\n",
      "|  476910|    5|2018|                300|             174.0|\n",
      "|  451878|    4|2018|                111|             64.38|\n",
      "|  481131|    4|2018|                144|            587.52|\n",
      "|  491883|    1|2019|                111|            574.98|\n",
      "|  493815|    1|2019|                333|            959.04|\n",
      "|  471891|    2|2019|                 75|363.96000000000004|\n",
      "|  469455|    6|2018|                663|            782.34|\n",
      "|  485268|   11|2018|                330|           2676.54|\n",
      "|  463575|    2|2018|                294|            650.88|\n",
      "|  467103|    5|2018|                444|1012.3199999999999|\n",
      "|  470085|   11|2018|               1335|3820.7999999999997|\n",
      "|  456855|    9|2018|                345|2798.6400000000003|\n",
      "|  480984|    5|2018|                255|            1040.4|\n",
      "|  456729|    9|2018|                189| 548.0999999999999|\n",
      "|  471576|   11|2018|                393|2533.8900000000003|\n",
      "|  447174|   10|2018|                132|             71.28|\n",
      "|  452193|    6|2018|                288|           1865.16|\n",
      "|  475188|    7|2018|                192|           1313.28|\n",
      "|  465654|    2|2019|                372|1106.9399999999998|\n",
      "+--------+-----+----+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import month, year, sum\n",
    "\n",
    "new_df.groupBy('ItemCode', month('TransactionTime').alias('month'), year('TransactionTime').alias('year')).agg(sum('NumberOfItemPurchased').alias('TotalPurchasedItems'), sum('Inventory Value').alias('Total Prices')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f12b318",
   "metadata": {},
   "source": [
    "## 7. Exploration of Users Transaction Behaviors throughout 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e17fe1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-------------------+--------+--------------------+---------------------+-----------+-----------+-----------------+-----------------+-----------------------------+\n",
      "|UserID|TransactionID|    TransactionTime|ItemCode|     ItemDescription|NumberOfItemPurchased|CostPerItem|    Country|TransactionCancel|  Inventory Value|NumberOfItemsPurchased_30days|\n",
      "+------+-------------+-------------------+--------+--------------------+---------------------+-----------+-----------+-----------------+-----------------+-----------------------------+\n",
      "|259455|      6076939|2018-07-21 17:49:00| 1528842|rose scent candle...|                   18|       5.87|    Bahrain|                0|           105.66|                          240|\n",
      "|259455|      6076939|2018-07-21 17:49:00|  476679|roses regency tea...|                   18|       4.08|    Bahrain|                0|            73.44|                          240|\n",
      "|259455|      6076939|2018-07-21 17:49:00| 1528842|vanilla scent can...|                   18|       5.87|    Bahrain|                0|           105.66|                          240|\n",
      "|259455|      6076939|2018-07-21 17:49:00|  476553|grow a flytrap or...|                   72|       1.73|    Bahrain|                0|           124.56|                          240|\n",
      "|259455|      6076939|2018-07-21 17:49:00| 1785840|s/4 pink flower c...|                   36|       2.28|    Bahrain|                0|            82.08|                          240|\n",
      "|259455|      6076939|2018-07-21 17:49:00| 1528842|ocean scent candl...|                   18|       5.87|    Bahrain|                0|           105.66|                          240|\n",
      "|259455|      6076939|2018-07-21 17:49:00|  475629|strawberry fairy ...|                   24|       6.84|    Bahrain|                0|           164.16|                          240|\n",
      "|259455|      6076939|2018-07-21 17:49:00|  476658|pink regency teac...|                   18|       4.08|    Bahrain|                0|            73.44|                          240|\n",
      "|259455|      6076939|2018-07-21 17:49:00|  476637|green regency tea...|                   18|       4.08|    Bahrain|                0|            73.44|                          240|\n",
      "|259623|      6046557|2018-06-23 17:29:00|  443541|set/10 red polkad...|                   72|       1.73|Unspecified|                0|           124.56|                          828|\n",
      "|259623|      6046557|2018-06-23 17:29:00|  445431|set of 72 skull p...|                   36|       2.01|Unspecified|                0|72.35999999999999|                          828|\n",
      "|259623|      6046557|2018-06-23 17:29:00|  443562|set/10 pink polka...|                   72|       1.73|Unspecified|                0|           124.56|                          828|\n",
      "|259623|      6046557|2018-06-23 17:29:00| 1784832|72 sweetheart fai...|                   72|       0.76|Unspecified|                0|            54.72|                          828|\n",
      "|259623|      6046557|2018-06-23 17:29:00|  461496|pack of 60 mushro...|                   72|       0.76|Unspecified|                0|            54.72|                          828|\n",
      "|259623|      6046557|2018-06-23 17:29:00|  445473|pack of 72 skull ...|                   72|       0.76|Unspecified|                0|            54.72|                          828|\n",
      "|259623|      6046557|2018-06-23 17:29:00| 1784811|60 teatime fairy ...|                   72|       0.76|Unspecified|                0|            54.72|                          828|\n",
      "|259623|      6046557|2018-06-23 17:29:00|  461454|set of 36 paisley...|                   36|       2.01|Unspecified|                0|72.35999999999999|                          828|\n",
      "|259623|      6046557|2018-06-23 17:29:00|  445452|pack of 72 retros...|                   72|       0.76|Unspecified|                0|            54.72|                          828|\n",
      "|259623|      6046557|2018-06-23 17:29:00|  443604|set/10 blue polka...|                   72|       1.73|Unspecified|                0|           124.56|                          828|\n",
      "|259623|      6046557|2018-06-23 17:29:00|  461433|set of 36 mushroo...|                   36|       2.01|Unspecified|                0|72.35999999999999|                          828|\n",
      "+------+-------------+-------------------+--------+--------------------+---------------------+-----------+-----------+-----------------+-----------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "new_df = spark.sql(\n",
    "    \"\"\"SELECT *, sum(NumberOfItemPurchased) OVER (\n",
    "        PARTITION BY UserID\n",
    "        ORDER BY TransactionTime\n",
    "        RANGE BETWEEN INTERVAL 29 DAYS PRECEDING AND CURRENT ROW\n",
    "     ) AS NumberOfItemsPurchased_30days\n",
    "     FROM df\"\"\")\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b3e392",
   "metadata": {},
   "source": [
    "## 8. Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035c1d9c",
   "metadata": {},
   "source": [
    "There are various approaches towards Recommendation Systems for this dataset: Collaborative Filtering, Content-Based Filtering, Customer Segmentation, or Deep Learning Approaches to name a few. However, the primary model for this notebook is Alternating Least Squares (ALS) available in PySpark's MLlib to perform Collaborative Filtering, using user-item interactions to recommend items that similar users have purchased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94b9bf9",
   "metadata": {},
   "source": [
    "This approach is efficient because:\n",
    "* ALS is designed to work well with sparse data, which is common in recommendation systems.\n",
    "* It's implemented in PySpark, so it can take advantage of distributed computing when it can.\n",
    "* It doesn't require extensive feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19d7cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4963273e",
   "metadata": {},
   "source": [
    "The number of total purchased items will be used as ratings in ALS. As these numbers can be smaller than 0, which is not ideal for ALS, negative values are replaced with 0.1 to present a really small number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97eaa325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------------+--------------+\n",
      "|UserID|ItemCode|TotalPurchased|AdjustedRating|\n",
      "+------+--------+--------------+--------------+\n",
      "|294840|  456855|           9.0|           9.0|\n",
      "|287889|  485163|         108.0|         108.0|\n",
      "|337029|  486297|           6.0|           6.0|\n",
      "|325353| 1734600|           9.0|           9.0|\n",
      "|313131|  451878|          36.0|          36.0|\n",
      "|283437|  474432|          12.0|          12.0|\n",
      "|274659|  442344|         150.0|         150.0|\n",
      "|297276|  475881|         120.0|         120.0|\n",
      "|329091|  474201|          36.0|          36.0|\n",
      "|340284| 1764609|          24.0|          24.0|\n",
      "|373653|  489678|           3.0|           3.0|\n",
      "|282555|  452319|           6.0|           6.0|\n",
      "|277452|  490371|           3.0|           3.0|\n",
      "|375543|  490392|           6.0|           6.0|\n",
      "|342930|  489321|           3.0|           3.0|\n",
      "|346983|  494886|          36.0|          36.0|\n",
      "|268422|  481383|          36.0|          36.0|\n",
      "|378504|  444465|          24.0|          24.0|\n",
      "|260148|  999390|          81.0|          81.0|\n",
      "|370944|  487830|          75.0|          75.0|\n",
      "+------+--------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data\n",
    "\n",
    "als_data = new_df.groupby(\"UserID\",\"ItemCode\").agg(sum(\"NumberOfItemPurchased\").alias(\"TotalPurchased\"))\n",
    "als_data = als_data.select(\n",
    "    col(\"UserID\").cast(\"integer\"),\n",
    "    col(\"ItemCode\").cast(\"integer\"),\n",
    "    col(\"TotalPurchased\").cast(\"float\")\n",
    ")\n",
    "\n",
    "# Adjust ratings based on Total Purchased Items\n",
    "als_data = als_data.withColumn(\n",
    "    \"AdjustedRating\",\n",
    "    when(col(\"TotalPurchased\") > 0, col(\"TotalPurchased\"))\n",
    "    .otherwise(0.1)  # Small positive value for cancelled transactions\n",
    ")\n",
    "\n",
    "als_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23e61dd",
   "metadata": {},
   "source": [
    "The dataset is splited into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "793db962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "(training, test) = als_data.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f1e808",
   "metadata": {},
   "source": [
    "Due to limit in computational power, parameter maxIter is set to small number (12) and coldStartStrategy is set to drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "507b7091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the recommendation model\n",
    "als = ALS(maxIter=12, regParam=0.8, userCol=\"UserID\", itemCol=\"ItemCode\",\n",
    "          ratingCol=\"AdjustedRating\", coldStartStrategy=\"drop\")\n",
    "\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be797c39",
   "metadata": {},
   "source": [
    "Root Mean Squared Error is used to evaluate the performance of the model as the ratings are the target prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67e01f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS RMSE: 38.58740921112539\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"AdjustedRating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"ALS RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5227869",
   "metadata": {},
   "source": [
    "Sample recommendation are calculated for some of the test users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44274585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample recommendations:\n",
      "+------+----------------------------------------------------------------------------------------------------------------+\n",
      "|UserID|recommendations                                                                                                 |\n",
      "+------+----------------------------------------------------------------------------------------------------------------+\n",
      "|259350|[{434028, 201.45451}, {1776516, 160.93117}, {1071294, 124.70993}, {466137, 111.27782}, {451899, 102.37344}]     |\n",
      "|259434|[{1894494, 160.00542}, {358764, 137.56435}, {1768452, 123.26226}, {997878, 122.450905}, {487893, 119.37599}]    |\n",
      "|259455|[{1071294, 75.6262}, {1782459, 75.436844}, {1783845, 73.60211}, {476553, 71.720436}, {316176, 65.726814}]       |\n",
      "|259665|[{434028, 217.74918}, {1776516, 208.9491}, {1071294, 118.79203}, {357252, 106.77239}, {736995, 99.60228}]       |\n",
      "|259770|[{358764, 139.78383}, {1783446, 135.78299}, {458430, 128.63123}, {468279, 118.69232}, {1777125, 117.97449}]     |\n",
      "|260211|[{1789830, 70.67389}, {1788675, 70.17304}, {358764, 67.83911}, {482685, 67.06963}, {444507, 66.328606}]         |\n",
      "|260610|[{444507, 150.42584}, {451395, 145.28873}, {451437, 137.99449}, {462819, 135.93202}, {482706, 131.97986}]       |\n",
      "|260694|[{434028, 235.7579}, {1789830, 231.72296}, {1071294, 197.39227}, {1776516, 178.44992}, {466137, 174.49538}]     |\n",
      "|261009|[{1776516, 397.40695}, {1071294, 313.48212}, {434028, 292.20337}, {357252, 269.98175}, {1722021, 260.35492}]    |\n",
      "|261030|[{1783446, 134.55016}, {1776516, 124.49318}, {1777125, 123.408936}, {1785399, 121.72162}, {1789830, 113.893585}]|\n",
      "|261282|[{1776516, 184.28914}, {1894494, 160.82304}, {434028, 152.83716}, {1789830, 136.10425}, {451395, 125.14112}]    |\n",
      "|261366|[{434028, 112.71133}, {1776516, 96.78881}, {1071294, 82.57425}, {462819, 81.38996}, {1783446, 79.810524}]       |\n",
      "|261702|[{358764, 126.36907}, {458430, 123.845215}, {1783446, 117.70116}, {1782459, 105.72637}, {1787079, 102.47503}]   |\n",
      "|261723|[{458430, 124.53298}, {466137, 107.6059}, {1071294, 101.94066}, {1777860, 101.23126}, {1785399, 95.59235}]      |\n",
      "|262059|[{1783446, 162.79938}, {1777125, 143.95215}, {358764, 130.41905}, {489489, 126.74197}, {476553, 117.826866}]    |\n",
      "|262101|[{434028, 228.62814}, {451899, 214.82103}, {451458, 201.1879}, {440916, 181.50735}, {339381, 176.9614}]         |\n",
      "|262143|[{434028, 439.42883}, {451395, 406.5555}, {1894494, 396.41302}, {1789830, 362.84103}, {478191, 290.3781}]       |\n",
      "|262290|[{1071294, 334.7961}, {434028, 309.73175}, {1789830, 271.02957}, {451458, 260.68848}, {478191, 251.82916}]      |\n",
      "|262374|[{434028, 161.0928}, {1783446, 114.780014}, {462819, 111.90291}, {451395, 111.58529}, {1071294, 106.21545}]     |\n",
      "|262836|[{1071294, 194.1339}, {997878, 184.5975}, {466137, 159.51648}, {1789830, 151.51637}, {1787751, 148.7717}]       |\n",
      "+------+----------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate recommendations\n",
    "user_recs = model.recommendForUserSubset(test.select('UserID').distinct(),5)\n",
    "\n",
    "# Show some recommendations\n",
    "print(\"Sample recommendations:\")\n",
    "user_recs.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df17462c",
   "metadata": {},
   "source": [
    "As we can see, the RMSE for the model is considerably large. To optimize the model, the number of iterations can be increased or regularization factor can be risen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6280ce6",
   "metadata": {},
   "source": [
    "As the number of features in this dataset is small, feature extraction can be performed to to increase the information to be used in the model. When the database is expanded further, more context can be included. For example, user profile can be added to study users' preferences, TransactionTime can be used to associate with seasonal products, or Country category can set as background information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306e8f4d",
   "metadata": {},
   "source": [
    "## Stopping the Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7fa8e48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
